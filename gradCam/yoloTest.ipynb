{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vgg19(weights = \"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoloModel = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImage = Image.open(\"C:/Users/josem/Documents/schoolWork/MQP/algonauts2023_transformers#2Leader/algonauts_2023_challenge_data/subj01/training_split/training_images/train-0001_nsd-00013.png\")\n",
    "testImage2 = Image.open(\"C:/Users/josem/Documents/schoolWork/MQP/algonauts2023_transformers#2Leader/algonauts_2023_challenge_data/subj01/training_split/training_images/train-0105_nsd-00868.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 2 persons, 3 elephants, 113.9ms\n",
      "Speed: 1.0ms preprocess, 113.9ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "results = yoloModel.predict(tsfms(testImage2)[None, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: array([[[118, 146, 112],\n",
       "         [139, 163, 134],\n",
       "         [126, 151, 124],\n",
       "         ...,\n",
       "         [204, 221, 248],\n",
       "         [190, 206, 233],\n",
       "         [182, 199, 227]],\n",
       " \n",
       "        [[ 99, 125,  96],\n",
       "         [110, 135, 107],\n",
       "         [118, 143, 115],\n",
       "         ...,\n",
       "         [198, 216, 243],\n",
       "         [189, 205, 231],\n",
       "         [180, 197, 224]],\n",
       " \n",
       "        [[ 86, 109,  89],\n",
       "         [100, 124, 105],\n",
       "         [106, 128, 110],\n",
       "         ...,\n",
       "         [201, 218, 245],\n",
       "         [196, 210, 241],\n",
       "         [183, 199, 229]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[212, 190, 144],\n",
       "         [217, 196, 149],\n",
       "         [214, 186, 145],\n",
       "         ...,\n",
       "         [138, 158,  96],\n",
       "         [123, 147,  85],\n",
       "         [129, 148,  83]],\n",
       " \n",
       "        [[205, 177, 130],\n",
       "         [203, 182, 137],\n",
       "         [205, 187, 143],\n",
       "         ...,\n",
       "         [143, 163, 101],\n",
       "         [138, 161, 101],\n",
       "         [138, 155,  94]],\n",
       " \n",
       "        [[203, 189, 135],\n",
       "         [195, 186, 131],\n",
       "         [183, 180, 122],\n",
       "         ...,\n",
       "         [146, 167, 100],\n",
       "         [167, 187, 132],\n",
       "         [167, 186, 132]]], dtype=uint8)\n",
       " orig_shape: (224, 224)\n",
       " path: 'image0.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 1.0211467742919922, 'inference': 113.91973495483398, 'postprocess': 1.9955635070800781}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 0.,  0., 20., 20., 20.], device='cuda:0')\n",
      "conf: tensor([0.7484, 0.6573, 0.4192, 0.3392, 0.2685], device='cuda:0')\n",
      "data: tensor([[4.8065e-04, 7.3214e+01, 4.2213e+01, 2.1964e+02, 7.4843e-01, 0.0000e+00],\n",
      "        [4.0169e+01, 9.1660e+01, 8.0540e+01, 2.0817e+02, 6.5734e-01, 0.0000e+00],\n",
      "        [8.6638e+01, 1.1993e+02, 1.4498e+02, 1.7621e+02, 4.1921e-01, 2.0000e+01],\n",
      "        [9.4496e+01, 7.5236e+01, 1.3859e+02, 1.0771e+02, 3.3922e-01, 2.0000e+01],\n",
      "        [8.0631e+01, 1.1377e+02, 1.7245e+02, 1.7408e+02, 2.6850e-01, 2.0000e+01]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (224, 224)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[ 21.1068, 146.4292,  42.2126, 146.4305],\n",
      "        [ 60.3546, 149.9126,  40.3704, 116.5062],\n",
      "        [115.8092, 148.0724,  58.3429,  56.2826],\n",
      "        [116.5426,  91.4713,  44.0941,  32.4704],\n",
      "        [126.5422, 143.9232,  91.8232,  60.3157]], device='cuda:0')\n",
      "xywhn: tensor([[0.0942, 0.6537, 0.1884, 0.6537],\n",
      "        [0.2694, 0.6693, 0.1802, 0.5201],\n",
      "        [0.5170, 0.6610, 0.2605, 0.2513],\n",
      "        [0.5203, 0.4084, 0.1968, 0.1450],\n",
      "        [0.5649, 0.6425, 0.4099, 0.2693]], device='cuda:0')\n",
      "xyxy: tensor([[4.8065e-04, 7.3214e+01, 4.2213e+01, 2.1964e+02],\n",
      "        [4.0169e+01, 9.1660e+01, 8.0540e+01, 2.0817e+02],\n",
      "        [8.6638e+01, 1.1993e+02, 1.4498e+02, 1.7621e+02],\n",
      "        [9.4496e+01, 7.5236e+01, 1.3859e+02, 1.0771e+02],\n",
      "        [8.0631e+01, 1.1377e+02, 1.7245e+02, 1.7408e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[2.1458e-06, 3.2685e-01, 1.8845e-01, 9.8056e-01],\n",
      "        [1.7933e-01, 4.0919e-01, 3.5955e-01, 9.2931e-01],\n",
      "        [3.8678e-01, 5.3541e-01, 6.4724e-01, 7.8667e-01],\n",
      "        [4.2186e-01, 3.3588e-01, 6.1870e-01, 4.8083e-01],\n",
      "        [3.5996e-01, 5.0788e-01, 7.6988e-01, 7.7715e-01]], device='cuda:0')\n",
      "2.1457672e-06 0.32684776 0.18845123 0.9805556\n",
      "1.50203705e-05 2.2879343 1.3191586 6.863889\n",
      "0.0 2.0 2.0 7.0\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    data = i.boxes\n",
    "    print(data)\n",
    "    # print(data.xywh.cpu().numpy())\n",
    "    boundingBoxData = data.xyxyn.cpu().numpy()\n",
    "    boundingBoxStartX, boundingBoxStartY, boundingBoxEndX, boundingBoxEndY = boundingBoxData[:, 0], boundingBoxData[:, 1], boundingBoxData[:, 2], boundingBoxData[:, 3]\n",
    "\n",
    "    transformedBoundingBoxStartX, transformedBoundingBoxStartY, transformedBoundingBoxEndX, transformedBoundingBoxEndY = boundingBoxStartX * 7, boundingBoxStartY * 7, boundingBoxEndX * 7, boundingBoxEndY * 7\n",
    "    startCellX = np.floor(transformedBoundingBoxStartX)\n",
    "    startCellY = np.floor(transformedBoundingBoxStartY)\n",
    "\n",
    "    endCellX = np.ceil(transformedBoundingBoxEndX)\n",
    "    endCellY = np.ceil(transformedBoundingBoxEndY)\n",
    "\n",
    "    print(boundingBoxStartX[0], boundingBoxStartY[0], boundingBoxEndX[0], boundingBoxEndY[0])\n",
    "    print(transformedBoundingBoxStartX[0], transformedBoundingBoxStartY[0], transformedBoundingBoxEndX[0], transformedBoundingBoxEndY[0])\n",
    "    print(startCellX[0], startCellY[0], endCellX[0], endCellY[0])\n",
    "\n",
    "    # boundingBoxData = data.xyxyn.cpu().numpy()\n",
    "    # boundingBoxStartX, boundingBoxStartY, boundingBoxWidth, boundingBoxHeight = boundingBoxData[:, 0], boundingBoxData[:, 1], boundingBoxData[:, 2], boundingBoxData[:, 3]\n",
    "\n",
    "    # transformedBoundingBoxStartX, transformedBoundingBoxStartY, transformedBoundingBoxWidth, transformedBoundingBoxHeight = boundingBoxStartX * 7, boundingBoxStartY * 7, boundingBoxWidth * 7, boundingBoxHeight * 7\n",
    "    # startCellX = np.floor(transformedBoundingBoxStartX)\n",
    "    # startCellY = np.floor(transformedBoundingBoxStartY)\n",
    "\n",
    "    # endCellX = np.ceil(transformedBoundingBoxStartX + transformedBoundingBoxWidth)\n",
    "    # endCellY = np.ceil(transformedBoundingBoxStartY + transformedBoundingBoxHeight)\n",
    "\n",
    "    # print(boundingBoxStartX[0], boundingBoxStartY[0], boundingBoxWidth[0], boundingBoxHeight[0])\n",
    "    # print(transformedBoundingBoxStartX[0], transformedBoundingBoxStartY[0], transformedBoundingBoxWidth[0], transformedBoundingBoxHeight[0])\n",
    "    # print(startCellX[0], startCellY[0], endCellX[0], endCellY[0])\n",
    "\n",
    "\n",
    "    # classPred = data.cls\n",
    "    # confidence = data.conf\n",
    "    # boundingBoxPos = data.xywh\n",
    "\n",
    "    # print(classPred)\n",
    "    # print(confidence)\n",
    "    # print(boundingBoxPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_boxes(image, boxes, class_names, confidence_threshold=0.5):\n",
    "    # Iterate through the detected boxes\n",
    "    for box in boxes:\n",
    "        x, y, width, height = box\n",
    "\n",
    "        # Filter boxes based on confidence threshold\n",
    "        # if confidence >= confidence_threshold:\n",
    "        # Convert relative coordinates to absolute coordinates\n",
    "        h, w, _ = image.shape\n",
    "        x = int(x * w)\n",
    "        y = int(y * h)\n",
    "        width = int(width * w)\n",
    "        height = int(height * h)\n",
    "\n",
    "        # Calculate box coordinates\n",
    "        x_min, y_min = x - width // 2, y - height // 2\n",
    "        x_max, y_max = x_min + width, y_min + height\n",
    "\n",
    "        # Draw bounding box on the image\n",
    "        color = (0, 255, 0)  # Green color\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "\n",
    "        # # Display class label and confidence\n",
    "        # label = f\"{class_names[class_index]}: {confidence:.2f}\"\n",
    "        # cv2.putText(image, label, (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Example bounding box data\n",
    "# Each box is represented as (class_index, confidence, x, y, width, height)\n",
    "bounding_boxes = [\n",
    "    (0, 0.8, 0.2, 0.3, 0.1, 0.15),  # Example box 1\n",
    "    (1, 0.9, 0.7, 0.5, 0.2, 0.25)   # Example box 2\n",
    "]\n",
    "\n",
    "# Example class names\n",
    "class_names = [\"Class A\", \"Class B\"]\n",
    "\n",
    "# Example image\n",
    "image_path = \"C:/Users/josem/Documents/schoolWork/MQP/algonauts2023_transformers#2Leader/algonauts_2023_challenge_data/subj01/training_split/training_images/train-0105_nsd-00868.png\"\n",
    "original_image = cv2.imread(image_path)\n",
    "original_image = cv2.resize(original_image, (224, 224))\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "image_with_boxes = draw_boxes(original_image.copy(), boundingBoxData, class_names)\n",
    "\n",
    "# Display the original and annotated images\n",
    "cv2.imshow(\"Original Image\", original_image)\n",
    "cv2.imshow(\"Image with Bounding Boxes\", image_with_boxes)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIVGG(torch.nn.Module):\n",
    "    def __init__(self, numClasses: int, numROIs: int, cocoVGGWeights:str = None, device: str = \"cuda\"):\n",
    "        super(ROIVGG, self).__init__()\n",
    "        self.cocoVgg19 = CocoVGG(numClasses)\n",
    "        if cocoVGGWeights is not None:\n",
    "            self.cocoVgg19.load_state_dict(torch.load(cocoVGGWeights, map_location=device))\n",
    "            \n",
    "        self.convFeatures = self.cocoVgg19.features[:36]\n",
    "        self.maxPool = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False)\n",
    "        self.adaptiveAvgPool = torch.nn.AdaptiveAvgPool2d(output_size = (7, 7))\n",
    "        self.cocoClassifier = self.cocoVgg19.classifier\n",
    "        self.roiClassifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features = 25088, out_features = 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(in_features = 4096, out_features = 1024),#1024\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(in_features = 1024, out_features = numROIs)\n",
    "        )\n",
    "        self.gradients = None\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "    def forward(self, img):\n",
    "        extractedFeatures = self.convFeatures(img)\n",
    "        # hook = extractedFeatures.register_hook(self.activations_hook)\n",
    "        intermediateOutput = self.maxPool(extractedFeatures)\n",
    "        intermediateOutput = self.adaptiveAvgPool(intermediateOutput)\n",
    "        intermediateOutput = torch.flatten(intermediateOutput, 1)\n",
    "        return self.cocoClassifier(intermediateOutput), self.roiClassifier(intermediateOutput)\n",
    "    def get_activation_gradient(self):\n",
    "        return self.gradients\n",
    "    def get_activations(self, img):\n",
    "        return self.convFeatures(img)\n",
    "    def __str__(self):\n",
    "        return \"\"\n",
    "    def __repr__(self):\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 5 (195062993.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.yolo = YOLO(\"yolov8n.pt\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 5\n"
     ]
    }
   ],
   "source": [
    "class roiVGGYolo(torch.nn.Module):\n",
    "    def __init__(self, numClasses: int, numROIs: int, roiVGGWeights: str = None, device = \"cuda\"):\n",
    "        super(roiVGGYolo, self).__init__()\n",
    "        self.vgg = vgg19(weights = \"DEFAULT\")\n",
    "        for params in self.vgg.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "        self.yolo = YOLO(\"yolov8n.pt\")\n",
    "        for params in self.yolo.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def forward(self, img):\n",
    "        pooling = self.vgg.features(img)\n",
    "        pooling = self.vgg.avgpool(pooling)\n",
    "\n",
    "        yoloResults = self.yolo(img)\n",
    "\n",
    "        for result in yoloResults:\n",
    "            data = results.boxes\n",
    "            classPred = data.cls.cpu().numpy()\n",
    "            confidence = data.conf.cpu().numpy()\n",
    "            boundingBoxData = data.xywh.cpu().numpy()\n",
    "            boundingBoxStartX, boundingBoxStartY, boundingBoxWidth, boundingBoxHeight = boundingBoxData[:, 0], boundingBoxData[:, 1], boundingBoxData[:, 2], boundingBoxData[:, 3]\n",
    "\n",
    "            transformedBoundingBoxStartX, transformedBoundingBoxStartY, transformedBoundingBoxWidth, transformedBoundingBoxHeight = boundingBoxStartX / 32, boundingBoxStartY / 32, boundingBoxWidth / 32, boundingBoxHeight / 32\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8907f5995ab74a6cd5df9da2d2bcd12f57f5b23c9c38358337eeb837f01ad676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
